"""
Skills Embeddings Module

This module provides functionality for:
- Encoding skills using sentence transformers
- Automatic skill name matching (e.g., "Jira" -> "Atlassian JIRA Software")
- Ranking consultants based on skill similarity
- Computing average scores across multiple skills
"""

import numpy as np
import pandas as pd


# ============================================================================
# 1. CORE CLASSES
# ============================================================================

class SkillEmbeddingModel:
    """
    Wrapper around SentenceTransformer or any similar encoder.
    Provides clean numpy embeddings with normalization.
    """
    
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
    
    def encode(self, text):
        """
        Encode text and return a normalized numpy array.
        Works for BGE-m3, E5, MiniLM, etc.
        """
        emb = self.embedding_model.encode(text, convert_to_tensor=False)
        emb = np.array(emb, dtype=np.float32)
        
        # Normalize embedding (cosine similarity becomes dot product)
        norm = np.linalg.norm(emb)
        if norm == 0:
            return emb
        return emb / norm


# ============================================================================
# 2. UTILITY FUNCTIONS
# ============================================================================

def cosine_sim(a, b):
    """
    Robust cosine similarity between two vectors.
    """
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)


def load_embeddings(npz_file: str):
    """
    Load embeddings from the .npz file generated by save_skills_emb().
    
    Returns:
        tuple: (skill_embs, domain_embs, metadata)
    """
    data = np.load(npz_file, allow_pickle=True)
    return data["skill_embs"], data["domain_embs"], data["metadata"]


# ============================================================================
# 3. AUTOMATIC SKILL NAME MATCHING
# ============================================================================

def build_skill_tree(skills_embeddings_file):
    """
    Build a tree structure from all skills in the dataset for automatic matching.
    This allows matching partial names (e.g., "Jira") to full names (e.g., "Atlassian JIRA Software").
    """
    from .tree import TreeNode
    
    _, _, metadata = load_embeddings(skills_embeddings_file)
    
    # Get all unique skills
    unique_skills = {}
    for _, _, skill_name in metadata:
        if skill_name not in unique_skills:
            unique_skills[skill_name] = skill_name
    
    # Build tree
    root = TreeNode('', '')
    for skill_name in unique_skills.values():
        words = skill_name.split()
        words.append("<EOS/>")
        root.add_child(words, skill_name)
    
    root.prune()
    return root


def get_all_unique_skills(skills_embeddings_file):
    """Get all unique skill names from the embeddings file."""
    _, _, metadata = load_embeddings(skills_embeddings_file)
    unique_skills = set()
    for _, _, skill_name in metadata:
        unique_skills.add(skill_name)
    return sorted(list(unique_skills))


def find_best_skill_match(query_skill, skill_tree, all_skills_list):
    """
    Find the best matching skill name from the dataset using tree structure.
    
    Args:
        query_skill: The skill name to search for (e.g., "Jira")
        skill_tree: The tree structure built from all skills
        all_skills_list: List of all unique skill names in the dataset
    
    Returns:
        tuple: (best_match, match_type) where match_type is 'exact', 'contains', 'partial', or None
    """
    query_lower = query_skill.lower().strip()
    
    # Strategy 1: Try exact match (case-insensitive)
    for skill in all_skills_list:
        if skill.lower() == query_lower:
            return skill, 'exact'
    
    # Strategy 2: Try to find if query is contained in any skill name
    # This handles cases like "Jira" -> "Atlassian JIRA Software"
    query_words = set(query_lower.split())
    best_match = None
    best_score = 0
    
    for skill in all_skills_list:
        skill_lower = skill.lower()
        skill_words = set(skill_lower.split())
        
        # If all query words are in the skill name, it's a good match
        if query_words.issubset(skill_words):
            # Score based on how many words match and position
            score = len(query_words) / len(skill_words)
            
            # Strong bonus if query is a substring (exact phrase match)
            if query_lower in skill_lower:
                score += 1.0
            
            # Bonus if query words appear consecutively in the skill name
            query_phrase = " ".join(query_lower.split())
            if query_phrase in skill_lower:
                score += 0.5
            
            # Penalty if skill name is much longer (prefer closer matches)
            if len(skill_words) > len(query_words) * 2:
                score *= 0.7
            
            if score > best_score:
                best_score = score
                best_match = skill
    
    if best_match and best_score > 0.3:  # Only return if score is good enough
        return best_match, 'contains'
    
    # Strategy 3: Try using the tree structure for partial matching
    # Only use this if we haven't found a good match yet and query has multiple words
    if len(query_words) > 1:
        try:
            query_words_list = query_skill.lower().split()
            matched_skills = {}
            
            # Try matching different n-grams, prioritizing longer matches
            for ngram_len in range(min(len(query_words_list), 3), 1, -1):  # Try 3-word, 2-word matches
                for i in range(len(query_words_list) - ngram_len + 1):
                    ngram = " ".join(query_words_list[i:i+ngram_len])
                    try:
                        skill_tree.match_term(ngram)
                    except Exception as e:
                        matched_skill = str(e)
                        # Only accept if the matched skill contains most of the query words
                        matched_skill_words = set(matched_skill.lower().split())
                        if len(query_words.intersection(matched_skill_words)) >= len(query_words) * 0.7:
                            if matched_skill not in matched_skills:
                                matched_skills[matched_skill] = ngram_len
            
            if matched_skills:
                # Return the match with the longest ngram (most specific)
                best_tree_match = max(matched_skills.items(), key=lambda x: x[1])[0]
                return best_tree_match, 'partial'
        except:
            pass
    
    # No match found - return original query (will use embedding similarity)
    return query_skill, None


# ============================================================================
# 4. RANKING FUNCTIONS
# ============================================================================

def rank_consultants(npz_file: str,
                     query_skill: str,
                     model,
                     top_level=100,
                     is_required=True,
                     score_multiplier=2,
                     min_level=0.55,
                     debug=False,
                     exact_match_boost=False):
    """
    Rank consultants based on:
      - similarity score (cosine after normalization)
      - experience level (LEVEL_VAL)
      - required vs optional flag
    
    Args:
        npz_file: Path to the .npz file containing skill embeddings
        query_skill: The skill name to search for (already matched to dataset name)
        model: SkillEmbeddingModel instance
        top_level: Maximum experience level (default 100)
        is_required: Whether this skill is required
        score_multiplier: Multiplier for required skills
        min_level: Minimum similarity threshold
        debug: Whether to return debug information
        exact_match_boost: Whether to boost exact skill name matches
    
    Returns:
        dict: {consultant_id: score} or tuple (dict, debug_info) if debug=True
    """
    skill_embs, _, metadata = load_embeddings(npz_file)
    query_embedding = model.encode([query_skill])[0]
    
    consultant_scores = {}
    debug_info = []
    
    for i, (user_id, level_xp, skill_name) in enumerate(metadata):
        similarity = cosine_sim(skill_embs[i], query_embedding)
        
        # Store debug info for top similarities
        if debug and similarity > 0.2:
            debug_info.append((user_id, skill_name, similarity, level_xp))
        
        # Normalize experience: LEVEL_VAL between 0 and top_level
        level_factor = min(level_xp, top_level) / top_level
        
        # Soft similarity factor: 0 below threshold, smooth above
        similarity_factor = max(0, similarity - min_level) / (1 - min_level)
        
        # Combine both
        score = level_factor * similarity_factor
        
        # Boost if required skill
        if is_required:
            score *= score_multiplier
        
        # Additional boost for exact matches
        if exact_match_boost:
            skill_name_str = str(skill_name).lower()
            matched_skill_lower = query_skill.lower()
            if skill_name_str == matched_skill_lower:
                score *= 1.5  # 50% boost for exact matches
        
        # Keep highest score for each consultant
        consultant_scores[user_id] = max(consultant_scores.get(user_id, 0), score)
    
    # Sort consultants by their score
    sorted_consultants = sorted(consultant_scores.items(),
                                key=lambda x: x[1],
                                reverse=True)
    
    result = {c: s for c, s in sorted_consultants}
    
    # Return debug info if requested
    if debug:
        debug_info.sort(key=lambda x: x[2], reverse=True)
        # Return top 50 to catch consultants like 2433141 who might be below top 10
        return result, debug_info[:50]
    
    return result


# ============================================================================
# 5. MAIN API FUNCTIONS
# ============================================================================

def get_consultants_skills_score(job_profile, skill_model, skills_embeddings_file, configuration, debug=False):
    """
    Scores consultants across all skills defined in the job profile.
    Uses automatic skill matching to find best matches in the dataset.
    
    Args:
        job_profile: Dictionary containing job requirements (must have "technologies" key)
        skill_model: SkillEmbeddingModel instance
        skills_embeddings_file: Path to .npz file with skill embeddings
        configuration: Configuration dictionary with skill matching parameters
        debug: Whether to print debug information
    
    Returns:
        dict: {skill_name: {consultant_id: score}} or tuple with debug data if debug=True
    """
    scores = {}
    debug_data = {}
    skills = job_profile["technologies"]
    
    # Build skill tree and get all unique skills for automatic matching (only once)
    skill_tree = build_skill_tree(skills_embeddings_file)
    all_skills = get_all_unique_skills(skills_embeddings_file)
    
    for skill_req in skills:
        if not skill_req:  # empty dict or missing
            continue
        
        skill_name = skill_req["name"]
        skill_level = skill_req.get("level", 100)  # Default to 100 if not specified
        is_required = skill_req["required"]
        
        # Automatically find the best matching skill name
        matched_skill, match_type = find_best_skill_match(skill_name, skill_tree, all_skills)
        
        if debug and match_type:
            print(f"[DEBUG] Skill matching: '{skill_name}' -> '{matched_skill}' (match type: {match_type})")
        
        # Use the matched skill name for ranking
        # top_level should be the maximum experience level (default 100)
        # If skill_level is 0 or None, use default 100 to avoid division by zero
        top_level = skill_level if skill_level and skill_level > 0 else 100
        
        result = rank_consultants(
            skills_embeddings_file,
            matched_skill,
            skill_model,
            top_level=top_level,
            is_required=is_required,
            score_multiplier=configuration["skills"]["required_multiplier"],
            min_level=configuration["skills"]["min_level"],
            debug=debug,
            exact_match_boost=(match_type == 'exact')
        )
        
        if debug:
            scores[skill_name], debug_data[skill_name] = result
        else:
            scores[skill_name] = result
    
    if debug:
        return scores, debug_data
    return scores


def average_skills(consultant_scores: dict):
    """
    Compute mean score per consultant across all skills.
    
    Args:
        consultant_scores: Dictionary of {skill_name: {consultant_id: score}}
    
    Returns:
        dict: {consultant_id: average_score}
    """
    means = {}
    all_skills = list(consultant_scores.keys())
    
    if not all_skills:
        return {}
    
    # Get all unique consultant IDs across all skills (union)
    all_consultant_ids = set()
    for skill in all_skills:
        all_consultant_ids.update(consultant_scores[skill].keys())
    
    # Calculate average score for each consultant
    # Use .get() to return 0 for skills consultant doesn't match
    for user_id in all_consultant_ids:
        consultant_mean_score = [
            consultant_scores[skill].get(user_id, 0.0)
            for skill in all_skills
        ]
        means[user_id] = np.mean(consultant_mean_score)
    
    return means


def sort_skills(final_scores):
    """
    Return sorted list of (consultant_id, score) tuples.
    
    Args:
        final_scores: Dictionary of {consultant_id: score}
    
    Returns:
        list: Sorted list of (consultant_id, score) tuples, highest score first
    """
    return sorted(final_scores.items(), key=lambda x: x[1], reverse=True)


# ============================================================================
# 6. HELPER FUNCTIONS (for generating embeddings)
# ============================================================================

def save_skills_emb(input_csv: str, output_npz: str, model):
    """
    Re-encode skills and domains using the CURRENT embedding model,
    saving results into a .npz file that your ranking pipeline reads.
    
    This function is used to generate/update the embeddings file when:
    - You change the embedding model
    - You add new skills to the dataset
    - You want to regenerate embeddings with different settings
    
    Args:
        input_csv: Path to CSV file with columns: USER_ID, SKILLS_DSC, DOMAIN_DSC, LEVEL_VAL
        output_npz: Path to output .npz file
        model: SkillEmbeddingModel instance
    """
    df = pd.read_csv(input_csv)
    
    df['SKILLS_EMB'] = df['SKILLS_DSC'].apply(lambda x: model.encode([x])[0])
    df['DOMAIN_EMB'] = df['DOMAIN_DSC'].apply(lambda x: model.encode([x])[0])
    
    skill_embs = np.stack(df['SKILLS_EMB'].values)
    domain_embs = np.stack(df['DOMAIN_EMB'].values)
    metadata = df[['USER_ID', 'LEVEL_VAL', 'SKILLS_DSC']].values
    
    np.savez(output_npz, skill_embs=skill_embs, domain_embs=domain_embs, metadata=metadata)
